{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago Weather Forecasting: Model Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I shall explain how one can build models for weather forecasting and offer evalutation of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Scope Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed with modelling, we need to define what the scope of this project is.\n",
    "\n",
    "### _(1) What we are forecasting_\n",
    "\n",
    "The goal is to predict a number of weather characteristics in the future for the city of Chicago. We shall select the basic componenets of a weather situation that a layperson will be considering for planning purposes. Think about deciding whether to go on a Sunday picnic on the beach or go hiking.  \n",
    "\n",
    "With that in mind, the following quantities are chosen:\n",
    "\n",
    "- Temperature (in Farenheit degrees)\n",
    "- Wind Speed (in miles per hour) \n",
    "- Precipitation (whether or not there will be rain / snow or hail)\n",
    "- Clarity (whether or not the sky will be free of clouds)\n",
    "\n",
    "Of those, forecasting Temperature and Wind are **Regression** problems, while forecasting Precipitation or Clarity are **Binary Classification** problems.\n",
    "\n",
    "We shall attempt to predict weather for the following durations in advance:\n",
    "\n",
    "- 6 hours\n",
    "- 12 hours\n",
    "- 18 hours\n",
    "- 24 hours\n",
    "\n",
    "(Preliminary experimentation has proven longer term forecasts to be not feasible with the data available)\n",
    "\n",
    "### _(2)  Model Dataset_\n",
    "\n",
    "As explained previously, we shall be relying on US Government (NOAA) datasets containing **hourly** weather reports for the weather station in Chicago as well as nearby stations in the US Midwest. \n",
    "\n",
    "In this notebook we shall work with data for 10 years, 2011-2020 (inclusively) from the following locations:\n",
    "\n",
    "- Chicago, IL (target location)\n",
    "- Cedar Rapids, IA\n",
    "- Des Moines, IA\n",
    "- Rochester, MN\n",
    "- Quincy, IL\n",
    "- Madison, WI\n",
    "- St Louis, MO\n",
    "- Green Bay, WI\n",
    "- Lansing, MI\n",
    "\n",
    "Most of these locations are **West** of Chicago as we previously determined through correlation analysis that locations in that direction have much more effect on weather in Chicago than locations in other directions.\n",
    "\n",
    "Finally, we shall be using preprocessed reports rather than the non-intuitive raw NOAA reports. See the following  (Timestamp is followed by the 4 quantities we aim to forecast as well as a few more for demo purposes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>Temp</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>_is_precip</th>\n",
       "      <th>_is_clear</th>\n",
       "      <th>CloudCondition</th>\n",
       "      <th>WeatherType</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>_wind_dir_sin</th>\n",
       "      <th>_wind_dir_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>40.333333</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.720</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-3.420201e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.735</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-1.736482e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.750</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.750</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PartlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.760</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011-01-01 05:00:00</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyClear</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.770</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011-01-01 06:00:00</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.785</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2011-01-01 07:00:00</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.810</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2011-01-01 08:00:00</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.870</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2011-01-01 09:00:00</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.890</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-3.420201e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2011-01-01 10:00:00</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.910</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011-01-01 11:00:00</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.890</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2011-01-01 12:00:00</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.890</td>\n",
       "      <td>55.500000</td>\n",
       "      <td>-0.906308</td>\n",
       "      <td>-4.226183e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2011-01-01 13:00:00</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.900</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-1.736482e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2011-01-01 14:00:00</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.930</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2011-01-01 15:00:00</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.960</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-1.736482e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011-01-01 16:00:00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.990</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2011-01-01 17:00:00</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>LightSnow</td>\n",
       "      <td>30.030</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-3.420201e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2011-01-01 18:00:00</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>30.040</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2011-01-01 19:00:00</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PartlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>30.070</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-1.736482e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   DATE       Temp  WindSpeed  _is_precip  _is_clear  \\\n",
       "0   2011-01-01 00:00:00  40.333333       13.0           0          0   \n",
       "1   2011-01-01 01:00:00  37.000000       17.0           0          0   \n",
       "2   2011-01-01 02:00:00  36.000000       17.0           0          0   \n",
       "3   2011-01-01 03:00:00  32.000000       15.0           0          0   \n",
       "4   2011-01-01 04:00:00  31.000000       16.0           0          1   \n",
       "5   2011-01-01 05:00:00  28.000000       18.0           0          1   \n",
       "6   2011-01-01 06:00:00  27.500000       17.0           0          0   \n",
       "7   2011-01-01 07:00:00  25.000000       20.0           0          0   \n",
       "8   2011-01-01 08:00:00  23.000000       21.0           0          0   \n",
       "9   2011-01-01 09:00:00  21.000000       23.0           0          0   \n",
       "10  2011-01-01 10:00:00  20.000000       21.0           0          0   \n",
       "11  2011-01-01 11:00:00  19.000000       24.0           0          0   \n",
       "12  2011-01-01 12:00:00  20.000000       23.0           0          0   \n",
       "13  2011-01-01 13:00:00  19.000000       25.0           0          0   \n",
       "14  2011-01-01 14:00:00  19.000000       18.0           0          0   \n",
       "15  2011-01-01 15:00:00  19.000000       20.0           0          0   \n",
       "16  2011-01-01 16:00:00  18.000000       16.0           0          0   \n",
       "17  2011-01-01 17:00:00  16.000000       20.0           1          0   \n",
       "18  2011-01-01 18:00:00  16.000000       18.0           0          0   \n",
       "19  2011-01-01 19:00:00  15.000000       18.0           0          1   \n",
       "\n",
       "   CloudCondition      WeatherType  Pressure   Humidity  _wind_dir_sin  \\\n",
       "0          Cloudy  NoPrecipitation    29.720  71.333333      -0.939693   \n",
       "1          Cloudy  NoPrecipitation    29.735  70.000000      -0.984808   \n",
       "2          Cloudy  NoPrecipitation    29.750  70.000000      -0.866025   \n",
       "3    MostlyCloudy  NoPrecipitation    29.750  61.000000      -0.866025   \n",
       "4    PartlyCloudy  NoPrecipitation    29.760  61.000000      -0.866025   \n",
       "5     MostlyClear  NoPrecipitation    29.770  63.000000      -0.866025   \n",
       "6    MostlyCloudy  NoPrecipitation    29.785  67.500000      -0.866025   \n",
       "7    MostlyCloudy  NoPrecipitation    29.810  75.000000      -0.866025   \n",
       "8          Cloudy  NoPrecipitation    29.870  65.000000      -0.866025   \n",
       "9          Cloudy  NoPrecipitation    29.890  62.000000      -0.939693   \n",
       "10         Cloudy  NoPrecipitation    29.910  55.000000      -0.866025   \n",
       "11   MostlyCloudy  NoPrecipitation    29.890  57.000000      -0.866025   \n",
       "12   MostlyCloudy  NoPrecipitation    29.890  55.500000      -0.906308   \n",
       "13   MostlyCloudy  NoPrecipitation    29.900  57.000000      -0.984808   \n",
       "14   MostlyCloudy  NoPrecipitation    29.930  54.000000      -0.866025   \n",
       "15   MostlyCloudy  NoPrecipitation    29.960  51.000000      -0.984808   \n",
       "16   MostlyCloudy  NoPrecipitation    29.990  62.000000      -1.000000   \n",
       "17   MostlyCloudy        LightSnow    30.030  67.000000      -0.939693   \n",
       "18   MostlyCloudy  NoPrecipitation    30.040  61.500000      -0.965926   \n",
       "19   PartlyCloudy  NoPrecipitation    30.070  59.000000      -0.984808   \n",
       "\n",
       "    _wind_dir_cos  \n",
       "0   -3.420201e-01  \n",
       "1   -1.736482e-01  \n",
       "2   -5.000000e-01  \n",
       "3   -5.000000e-01  \n",
       "4   -5.000000e-01  \n",
       "5   -5.000000e-01  \n",
       "6   -5.000000e-01  \n",
       "7   -5.000000e-01  \n",
       "8   -5.000000e-01  \n",
       "9   -3.420201e-01  \n",
       "10  -5.000000e-01  \n",
       "11  -5.000000e-01  \n",
       "12  -4.226183e-01  \n",
       "13  -1.736482e-01  \n",
       "14  -5.000000e-01  \n",
       "15  -1.736482e-01  \n",
       "16  -1.836970e-16  \n",
       "17  -3.420201e-01  \n",
       "18  -2.588190e-01  \n",
       "19  -1.736482e-01  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../processed-data/noaa_2011-2020_chicago_PREPROC.csv')\n",
    "subset_df = df [['DATE', 'Temp', 'WindSpeed', '_is_precip', '_is_clear', 'CloudCondition', 'WeatherType', \n",
    "                 'Pressure', 'Humidity', '_wind_dir_sin', '_wind_dir_cos']]\n",
    "subset_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _(3)  Aggregated Forecasting_\n",
    "\n",
    "As explained previously, the datasets are chronological lists of hourly data points. What does it mean to forecast each of the target quantities, say, 12h, in advance?\n",
    "\n",
    "Predicting weather for a particular hour may not serve us particularly well. Consider the following situations: \n",
    "\n",
    "- let's say it is 11PM and we are considering a picnic at 11AM the next day. If it does not rain at 11AM but it does rain at 10AM or 1PM, the picnic is a bad idea.\n",
    "\n",
    "- similarly, if we are considering kayaking, if the wind is going to be 5mph at 11AM but 30mph at 2PM, we should reconsider\n",
    "\n",
    "*To address such concerns, we shall be attempting to forecast not weather for the exact target hour but rather some kind of **aggregation over an interval** centered around that hour.*\n",
    "\n",
    "In the code to follow we shall rely on something called **Aggregation Half Interval (AHI)**. For example, if AHI = 3 and the target hour is 11AM, we shall be considering the interval spanning 08AM to 02PM. \n",
    "\n",
    "Let us now come up with the definitions for what that means for each of the 4 forecasted quantities:\n",
    "\n",
    "| Quantity | AHI | Aggregation Rule |\n",
    "| --- | --- | --- |\n",
    "| Temperature | 1h | Average |\n",
    "| WindSpeed   | 2h | Average |\n",
    "| Precipitation | 3h | True if any element is True |\n",
    "| Clarity | 3h  | True if all elements are True |\n",
    "\n",
    "The first two rows for analog quantities are self explanatory: we are smoothing the prediction over an interval by averaging. Temperature has a smaller interval as it is much more directly dependent on time of day than wind.\n",
    "\n",
    "The last two rows for binary quantities say that if *any* hour during the interval is Rainy or Not Fair, the resulting forecast too is Rainy or Not Fair. As per the situation described above, if it rains anywhere close to the hour for which we are forecasting, we'll get wet. Similarly, if it is not fair anywhere close to that hour, our sun tanning won't go well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Learning Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) _Merge Data from All Locations_\n",
    "\n",
    "We need to do a JOIN on all the weather reports whose data we'll be feeding into our models. The following functions perform the merge and drop any irrelevant columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFeatureSet(targetLocationFile, adjacentLocationFiles, predictedVariable, featuresToUse):\n",
    "    target_df = pd.read_csv(targetLocationFile, parse_dates=['DATE'])\n",
    "    target_df = dropUnusedColumns(target_df, predictedVariable, featuresToUse)\n",
    "    merged_df = target_df\n",
    "    suffix_no = 1\n",
    "\n",
    "    # Merge adjacent location files one by one relying on DATE\n",
    "    for adjacentLocationFile in adjacentLocationFiles:\n",
    "        adjacent_df = pd.read_csv(adjacentLocationFile, parse_dates=['DATE'])\n",
    "        adjacent_df = dropUnusedColumns(adjacent_df, predictedVariable, featuresToUse)\n",
    "\n",
    "        #Take control of column name suffix in the dataset being merged in\n",
    "        adjacent_df = adjacent_df.add_suffix(str(suffix_no))\n",
    "        adjacent_df = adjacent_df.rename(columns = {\"DATE{}\".format(suffix_no) :'DATE'})\n",
    "        merged_df = pd.merge(merged_df, adjacent_df, on='DATE')\n",
    "        suffix_no = suffix_no + 1\n",
    "\n",
    "    # DATE column is of no use in the modelling stage (we only needed it for merging)\n",
    "    merged_df = merged_df.drop(columns=['DATE'])\n",
    "    return merged_df\n",
    "\n",
    "#======================================================================\n",
    "# Keep only the DATE column, the variable we are predicting and the variables that we use for prediction\n",
    "def dropUnusedColumns(df, predictedVariable, featuresToUse):\n",
    "    all_columns = featuresToUse.copy()\n",
    "    all_columns.append('DATE')\n",
    "    all_columns.append(predictedVariable)\n",
    "    df = df[all_columns]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_wind_dir_sin</th>\n",
       "      <th>_wind_dir_cos</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>_wind_dir_sin1</th>\n",
       "      <th>_wind_dir_cos1</th>\n",
       "      <th>WindSpeed1</th>\n",
       "      <th>_wind_dir_sin2</th>\n",
       "      <th>_wind_dir_cos2</th>\n",
       "      <th>WindSpeed2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-0.342020</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.802123</td>\n",
       "      <td>-0.597159</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>23.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-0.173648</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-0.342020</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-0.173648</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-0.342020</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-0.342020</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _wind_dir_sin  _wind_dir_cos  WindSpeed  _wind_dir_sin1  _wind_dir_cos1  \\\n",
       "0      -0.939693      -0.342020       13.0       -0.802123       -0.597159   \n",
       "1      -0.984808      -0.173648       17.0       -0.642788       -0.766044   \n",
       "2      -0.866025      -0.500000       17.0       -0.766044       -0.642788   \n",
       "3      -0.866025      -0.500000       15.0       -0.866025       -0.500000   \n",
       "4      -0.866025      -0.500000       16.0       -0.866025       -0.500000   \n",
       "\n",
       "   WindSpeed1  _wind_dir_sin2  _wind_dir_cos2  WindSpeed2  \n",
       "0   23.666667       -0.866025       -0.500000        23.5  \n",
       "1   25.000000       -0.939693       -0.342020        24.0  \n",
       "2   23.000000       -0.984808       -0.173648        22.0  \n",
       "3   23.000000       -0.939693       -0.342020        22.0  \n",
       "4   23.000000       -0.939693       -0.342020        16.0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset = buildFeatureSet(\n",
    "    '../processed-data/noaa_2011-2020_chicago_PREPROC.csv',\n",
    "    ['../processed-data/noaa_2011-2020_cedar-rapids_PREPROC.csv', \n",
    "         '../processed-data/noaa_2011-2020_des-moines_PREPROC.csv'],\n",
    "    predictedVariable='WindSpeed',\n",
    "    featuresToUse = ['_wind_dir_sin', '_wind_dir_cos']\n",
    "    )\n",
    "featureset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of 3 variables of interest: `WindSpeed` (predicted) as well as `_wind_dir_sin` and `_wind_dir_cos` (to be used for predicting). As you can see, the dataset above has these variables repeated 3 times, once for each location. This merged kind of dataset will be used going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) _Split and Normalize the Data_\n",
    "\n",
    "Before we can train models we must split the data into the 3 subsets:\n",
    "\n",
    "- *Training*: the actual data that we'll be training on. This is the largest subset.\n",
    "- *Validation*: the dataset to be used for model tuning during training to check the model periodically\n",
    "- *Testing*: the dataset that will be hidden from the model training process and be used for final model evaluation\n",
    "\n",
    "Of course, we'll also need to normalize the features on which we are training to avoid algorithm execution issues like gradient explosion. The following code achieves both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_wind_dir_sin</th>\n",
       "      <th>_wind_dir_cos</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>_wind_dir_sin1</th>\n",
       "      <th>_wind_dir_cos1</th>\n",
       "      <th>WindSpeed1</th>\n",
       "      <th>_wind_dir_sin2</th>\n",
       "      <th>_wind_dir_cos2</th>\n",
       "      <th>WindSpeed2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.188696</td>\n",
       "      <td>-0.466556</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-1.144549</td>\n",
       "      <td>-0.762454</td>\n",
       "      <td>2.369524</td>\n",
       "      <td>-1.332160</td>\n",
       "      <td>-0.567817</td>\n",
       "      <td>2.530257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.256421</td>\n",
       "      <td>-0.236236</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.902651</td>\n",
       "      <td>-0.987495</td>\n",
       "      <td>2.595942</td>\n",
       "      <td>-1.448306</td>\n",
       "      <td>-0.362728</td>\n",
       "      <td>2.621978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.078108</td>\n",
       "      <td>-0.682661</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1.089776</td>\n",
       "      <td>-0.823255</td>\n",
       "      <td>2.256315</td>\n",
       "      <td>-1.519435</td>\n",
       "      <td>-0.144148</td>\n",
       "      <td>2.255092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.078108</td>\n",
       "      <td>-0.682661</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.241564</td>\n",
       "      <td>-0.632990</td>\n",
       "      <td>2.256315</td>\n",
       "      <td>-1.448306</td>\n",
       "      <td>-0.362728</td>\n",
       "      <td>2.255092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.078108</td>\n",
       "      <td>-0.682661</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-1.241564</td>\n",
       "      <td>-0.632990</td>\n",
       "      <td>2.256315</td>\n",
       "      <td>-1.448306</td>\n",
       "      <td>-0.362728</td>\n",
       "      <td>1.154431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _wind_dir_sin  _wind_dir_cos  WindSpeed  _wind_dir_sin1  _wind_dir_cos1  \\\n",
       "0      -1.188696      -0.466556       13.0       -1.144549       -0.762454   \n",
       "1      -1.256421      -0.236236       17.0       -0.902651       -0.987495   \n",
       "2      -1.078108      -0.682661       17.0       -1.089776       -0.823255   \n",
       "3      -1.078108      -0.682661       15.0       -1.241564       -0.632990   \n",
       "4      -1.078108      -0.682661       16.0       -1.241564       -0.632990   \n",
       "\n",
       "   WindSpeed1  _wind_dir_sin2  _wind_dir_cos2  WindSpeed2  \n",
       "0    2.369524       -1.332160       -0.567817    2.530257  \n",
       "1    2.595942       -1.448306       -0.362728    2.621978  \n",
       "2    2.256315       -1.519435       -0.144148    2.255092  \n",
       "3    2.256315       -1.448306       -0.362728    2.255092  \n",
       "4    2.256315       -1.448306       -0.362728    1.154431  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def normalizeData(trainDf, valDf,  testDf, predictedVariable, featuresToUse, adjacentLocationCount):\n",
    "\n",
    "    columns_to_normalize = featuresToUse.copy()\n",
    "\n",
    "    prefixes_to_normalize = featuresToUse.copy()\n",
    "    prefixes_to_normalize.append(predictedVariable)\n",
    "    for loc in range(1, 1 + adjacentLocationCount):\n",
    "        for prefix in prefixes_to_normalize:\n",
    "            columns_to_normalize.append(\"{}{}\".format(prefix, loc))\n",
    "\n",
    "    # Normalize input data but not the target variable\n",
    "    train_mean = trainDf[columns_to_normalize].mean()\n",
    "    train_std = trainDf[columns_to_normalize].std()\n",
    "\n",
    "    trainDf[columns_to_normalize] = (trainDf[columns_to_normalize] - train_mean) / train_std\n",
    "    valDf[columns_to_normalize] = (valDf[columns_to_normalize] - train_mean) / train_std\n",
    "    testDf[columns_to_normalize] = (testDf[columns_to_normalize] - train_mean) / train_std\n",
    "\n",
    "    return trainDf, valDf, testDf\n",
    "\n",
    "\n",
    "# Split the data: 6 years for training, 2 for validation & 2 for testing\n",
    "n = len(featureset)\n",
    "train_df = featureset[0 : int(n*0.60)]\n",
    "val_df = featureset[int(n*0.60) : int(n*0.80)]\n",
    "test_df = featureset[int(n*0.80) : ]\n",
    "\n",
    "# Normalize input data\n",
    "train_df, val_df, test_df = normalizeData(train_df, val_df, test_df, \n",
    "                                          'WindSpeed', ['_wind_dir_sin', '_wind_dir_cos'], 2)\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) _Prepare the Data for TensorFlow_\n",
    "\n",
    "We still have further to go before we can use TensorFlow to build models. \n",
    "\n",
    "First, we need to create a Sliding Window type data structure containing a number of observations in the Past. For example if we are forecasting _Temperature_ 12h in advance and we want to look back 3 hours, we need `Temperature[-12h], Temperature[-13h], Temperature[-14h]` all in one row.\n",
    "\n",
    "Second, TensorFlow is quite particular about what form the input data should take:\n",
    "\n",
    "_Typically data in TensorFlow is packed into arrays where the outermost index is across examples (the \"batch\" dimension). The middle indices are the \"time\" or \"space\" (width, height) dimension(s). The innermost indices are the features_ (see https://www.tensorflow.org/tutorials/structured_data/time_series).\n",
    "\n",
    "The following class borrowed from the manual above takes the Pandas dataset and massages it into a Sliding Window tensor of the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "class WindowGenerator():\n",
    "\n",
    "    def __init__(self, \n",
    "        input_width, # Lookback Window (hours into the past to base predictions on)\n",
    "        label_width, # Aggregation Interval (how many hours of data we'll be predicting)\n",
    "        shift, # How many hours in advance we'll be predicting\n",
    "        train_df, val_df, test_df, # Training, Validation and Testing sets\n",
    "        label_columns=None):\n",
    "\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "          self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "          data=data,\n",
    "          targets=None,\n",
    "          sequence_length=self.total_window_size,\n",
    "          sequence_stride=1,\n",
    "          shuffle=False,\n",
    "          batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now use the above to demonstrate generation of Tensorflow Datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape (batch, time, features): (32, 12, 9)\n",
      "Labels shape (batch, time, features): (32, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "wg = WindowGenerator(\n",
    "    input_width = 12, # Take 12h of history into account\n",
    "    label_width = 5,  # Corresponds to aggreagation half-interval of 2h\n",
    "    shift = 4, # Forecast 6 hours in Advance (6 - (AHI=2) = 4)\n",
    "    label_columns=['WindSpeed'],\n",
    "    train_df = train_df, val_df = val_df, test_df = test_df # Create Tensorflow datasets for all 3 subsets\n",
    ")\n",
    "\n",
    "for example_inputs, example_labels in wg.train.take(1):\n",
    "    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "    print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those shapes are explained as follows:\n",
    "\n",
    "- 32 is the batch size (Tensorflow trains on data in batches)\n",
    "- 12 is the number of hours: for Inputs it is 12 because we are looking 12 hours back while for Outputs it is 5 because we are aggregating over 5h \n",
    "- 9 is the number of input features: `WindSpeed`, `_wind_dir_sin` and `_wind_dir_cos` for each of the 3 locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) _Our First Model: Linear_\n",
    "\n",
    "It is time to build our very first model! It is the simplest one that there is, the `Linear` kind. The approach that we'll take is also borrowed from https://www.tensorflow.org/tutorials/structured_data/time_series . The goal is to predict *all* target labels in the aggregation interval in one shot. So in the case above, where the interval is 5, we'll be predicting 5 values in chronological order. \n",
    "\n",
    "We are going to make our model creation and evaluation code work for both Binary Classification and Regression problems as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGRESSION_METRICS = [ tf.keras.metrics.RootMeanSquaredError()]\n",
    "CLASSIFICATION_METRICS =[tf.keras.metrics.Recall(name=\"recall\"), tf.keras.metrics.Precision()]\n",
    "def getActivationLossAndMetrics(isBinary):\n",
    "    activation, loss, metrics = \"linear\", 'mean_squared_error', REGRESSION_METRICS\n",
    "    if isBinary:\n",
    "        activation, loss, metrics = \"sigmoid\", tf.keras.losses.BinaryCrossentropy(), CLASSIFICATION_METRICS\n",
    "\n",
    "    return activation, loss, metrics\n",
    "\n",
    "\n",
    "def buildLinearModel(isBinary, label_width):\n",
    "    _activation, _loss, _metrics = getActivationLossAndMetrics(isBinary)\n",
    "    model = tf.keras.Sequential([\n",
    "        # Use all time steps\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        # Note that there are no hidden layers, only the output one, which makes the model linear.\n",
    "        tf.keras.layers.Dense(units=label_width, activation = _activation, kernel_initializer=tf.initializers.zeros()),\n",
    "        tf.keras.layers.Reshape([label_width, 1]),\n",
    "    ])\n",
    "    model.compile(loss=_loss, optimizer='adam', metrics = _metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) _Model Evaluation_\n",
    "\n",
    "Before we proceed to actually train our model we need to establish the criteria based on which we'll evaluate it. \n",
    "\n",
    "This is a Regression model so we'll use the following metrics:\n",
    "\n",
    "- Root Mean Squared Error\n",
    "- Absolute Mean Error (less affected by outliers)\n",
    "- R2 Score\n",
    "- MAPE (Mean Absolute Percentage Error)\n",
    "\n",
    "While we are on the subject, we shall also define the Binary Classification Metrics (we'll use them later):\n",
    "\n",
    "- Recall\n",
    "- Precision\n",
    "- F1 Score\n",
    "- MCC (Matthew Coefficient, arguably the most balanced measure of Binary Classification performance)\n",
    "\n",
    "Finally, we'll need some extra code to evaluate the model. Remember, our models are predicting a sequence of values over a time interval, whereas our application calls for aggregating values over that interval. The code below will handle that as well by parsing through the Tensors returned by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import math\n",
    "\n",
    "# https://www.statology.org/mape-python/\n",
    "def calcMape(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred) \n",
    "    actual[abs(actual )< 0.1] = 0.1 # A meh hack to avoid division by 0\n",
    "\n",
    "    return np.mean(np.abs((actual - pred) / actual )) * 100\n",
    "\n",
    "# https://kodify.net/python/math/truncate-decimals/\t\n",
    "def truncate(number, decimals=0):\n",
    "    \"\"\"\n",
    "    Returns a value truncated to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer.\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more.\")\n",
    "    elif decimals == 0:\n",
    "        return math.trunc(number)\n",
    "\n",
    "    factor = 10.0 ** decimals\n",
    "    return math.trunc(number * factor) / factor\n",
    "\n",
    "def evaluateClassificationModel(model, testSet, options = []):\n",
    "\n",
    "    predicted_labels =(model.predict(testSet, verbose = 1) > 0.5).astype(\"int32\")\n",
    "    true_labels = np.concatenate([y for x, y in testSet], axis=0)\n",
    "\n",
    "    assert len(predicted_labels) == len (true_labels)\n",
    "\n",
    "    # We are forecasting for a number of hours: \n",
    "    # aggregate each forecast series using the \"True iff 1 or more is True\" rule (default) or\n",
    "    # \"True iff all True\" (option specified)\n",
    "    predicted_agg = []\n",
    "    true_agg = []\n",
    "    for i in range(0, len(predicted_labels)):\n",
    "        predicted_i = predicted_labels[i].flatten()\n",
    "        true_i = true_labels[i].flatten()\n",
    "\n",
    "        if not \"TrueIfAllTrue\" in options:\n",
    "            predicted_i_agg = 1 if sum(predicted_i) > 0 else 0\n",
    "            true_i_agg = 1 if sum(true_i) > 0 else 0\n",
    "        else:\n",
    "            predicted_i_agg = 1 if sum(predicted_i) == len(predicted_i) else 0\n",
    "            true_i_agg = 1 if sum(true_i) == len(true_i) else 0\n",
    "\n",
    "        predicted_agg.append(predicted_i_agg)\n",
    "        true_agg.append(true_i_agg)\n",
    "\n",
    "    recall = truncate(recall_score(true_agg, predicted_agg), 2)\n",
    "    precision = truncate(precision_score(true_agg, predicted_agg), 2)\n",
    "    f1 = truncate(f1_score(true_agg, predicted_agg), 2)\n",
    "    mcc = truncate(matthews_corrcoef(true_agg, predicted_agg), 2)\n",
    "\n",
    "    print(confusion_matrix(true_agg, predicted_agg))\n",
    "    print(\"Recall = {}, Precision = {}, F1 = {}, MCC = {}\".format(recall, precision, f1, mcc))\n",
    "\n",
    "    return {\n",
    "        \"Recall\": recall,\n",
    "        \"Precision\": precision, \n",
    "        \"F1:\" : f1,\n",
    "        \"MCC:\": mcc\n",
    "    }\t\n",
    "\n",
    "def evaluateRegressionModel(model, testSet, options=[]):\n",
    "    predicted_values = model.predict(testSet)\n",
    "    true_values = np.concatenate([y for x, y in testSet], axis=0)\n",
    "\n",
    "    assert len(predicted_values) == len (true_values)\n",
    "\n",
    "    predicted_agg = []\n",
    "    true_agg = []\n",
    "    for i in range(0, len(predicted_values)):\n",
    "        predicted_i = predicted_values[i].flatten()\n",
    "        true_i = true_values[i].flatten()\n",
    "\n",
    "        predicted_i_agg, true_i_agg = np.mean(predicted_i), np.mean(true_i)\n",
    "        predicted_agg.append(predicted_i_agg)\n",
    "        true_agg.append(true_i_agg)\n",
    "\n",
    "    rmse = truncate(math.sqrt(mean_squared_error(true_agg, predicted_agg)), 2)\n",
    "    mae = truncate(mean_absolute_error(true_agg, predicted_agg), 2)\n",
    "    r2 = truncate(r2_score(true_agg, predicted_agg), 2)\n",
    "    mape = truncate(calcMape(true_agg, predicted_agg), 2) \n",
    "\n",
    "    print(\"R2 = {}, RMSE = {}, MAE = {}, MAPE = {}%\".format(r2, rmse, mae, mape))\n",
    "    return {\n",
    "        'R2' : r2,\n",
    "        'RMSE' : rmse,\n",
    "        'MAE' : mae,\n",
    "        'MAPE' : \"{}%\".format(mape)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) _A Note on Avoiding Overfitting_\n",
    "\n",
    "Overfitting is a common problem in Machine Learning, especially with Neural Networks. Given enough model complexity and iteration epochs, learing algorithms tend to learn the Training Dataset perfectly but generalize poorly to previously unseen (Test/Validation) data. \n",
    "\n",
    "We shall use the `EarlyStopping` Callback mechanism provided by TF. The idea is that once the target metric stops improving on the Validation Dataset we stop the training. We then go with the learned model weights  for which that target metic is optimal on the Validation Dataset. This is the simplest regularization technique out there and for the purposes of this exercise it has proven to be adequate. \n",
    "\n",
    "For Regression problems we shall use `RMSE` as the target metric and for Binary Classification problems we shall use `Recall`. The idea behind the latter is that both Precipitation and Clarity are much less common than their opposites (Chicago is not exactly known for wonderful weather) and we shall stir the model towards avoiding False Negatives. That is, we'll try to catch as many Rainy / Clear days as we can worrying less about mistakenly flagging not rainy and not clear days.\n",
    "\n",
    "Of course, Overfitting is less of a concern for the Linear model that we'll be demonstrating first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) _Fit our First Model and Evaluate the Result_\n",
    "\n",
    "We are now ready to fit our first model to try and predict Temperature. \n",
    "\n",
    "Note that we'll configure Early Stopping to prevent us from Overfitting the Training Set as well as save on training time when the returns become too deminishing. See the above section for explainations.\n",
    "\n",
    "First, though, build a less skimpy featureset than what we demoed above to get better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the Previous steps for a better dataset\n",
    "features_to_use = ['_day_sin', '_day_cos', '_hour_sin', '_hour_cos', 'DewPoint', 'WindSpeed', '_cloud_intensity']\n",
    "featureset = buildFeatureSet(\n",
    "    '../processed-data/noaa_2011-2020_chicago_PREPROC.csv',\n",
    "    ['../processed-data/noaa_2011-2020_cedar-rapids_PREPROC.csv', \n",
    "     '../processed-data/noaa_2011-2020_madison_PREPROC.csv',\n",
    "     '../processed-data/noaa_2011-2020_des-moines_PREPROC.csv',\n",
    "    '../processed-data/noaa_2011-2020_rochester_PREPROC.csv'],\n",
    "    predictedVariable='Temp',\n",
    "    featuresToUse = features_to_use\n",
    "    )\n",
    "n = len(featureset)\n",
    "train_df = featureset[0 : int(n*0.60)]\n",
    "val_df = featureset[int(n*0.60) : int(n*0.80)]\n",
    "test_df = featureset[int(n*0.80) : ]\n",
    "train_df, val_df, test_df = normalizeData(train_df, val_df, test_df, \n",
    "                                          'Temp', features_to_use, 4)\n",
    "window = WindowGenerator(\n",
    "    input_width = 2, # Take 2h of history into account\n",
    "    label_width = 3,  # Corresponds to aggreagation half-interval of 1h\n",
    "    shift = 5, # Forecast 6 hours in Advance (6 - (AHI=1) = 5)\n",
    "    label_columns=['Temp'],\n",
    "    train_df = train_df, val_df = val_df, test_df = test_df # Create Tensorflow datasets for all 3 subsets\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use that larger dataset to do the modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1644/1644 [==============================] - 1s 687us/step - loss: 629.5239 - root_mean_squared_error: 24.2431 - val_loss: 19.4044 - val_root_mean_squared_error: 4.4050\n",
      "Epoch 2/20\n",
      "1644/1644 [==============================] - 1s 706us/step - loss: 19.6446 - root_mean_squared_error: 4.4304 - val_loss: 18.1651 - val_root_mean_squared_error: 4.2621\n",
      "Epoch 3/20\n",
      "1644/1644 [==============================] - 1s 764us/step - loss: 19.0656 - root_mean_squared_error: 4.3645 - val_loss: 17.6788 - val_root_mean_squared_error: 4.2046\n",
      "Epoch 4/20\n",
      "1644/1644 [==============================] - 1s 697us/step - loss: 18.5970 - root_mean_squared_error: 4.3104 - val_loss: 17.1504 - val_root_mean_squared_error: 4.1413\n",
      "Epoch 5/20\n",
      "1644/1644 [==============================] - 1s 710us/step - loss: 18.0705 - root_mean_squared_error: 4.2488 - val_loss: 16.6797 - val_root_mean_squared_error: 4.0841\n",
      "Epoch 6/20\n",
      "1644/1644 [==============================] - 1s 772us/step - loss: 17.5628 - root_mean_squared_error: 4.1885 - val_loss: 16.2896 - val_root_mean_squared_error: 4.0360\n",
      "Epoch 7/20\n",
      "1644/1644 [==============================] - 1s 666us/step - loss: 17.1332 - root_mean_squared_error: 4.1368 - val_loss: 15.9818 - val_root_mean_squared_error: 3.9977\n",
      "Epoch 8/20\n",
      "1644/1644 [==============================] - 1s 746us/step - loss: 16.7845 - root_mean_squared_error: 4.0943 - val_loss: 15.7391 - val_root_mean_squared_error: 3.9673\n",
      "Epoch 9/20\n",
      "1644/1644 [==============================] - 1s 655us/step - loss: 16.5010 - root_mean_squared_error: 4.0594 - val_loss: 15.5444 - val_root_mean_squared_error: 3.9426\n",
      "Epoch 10/20\n",
      "1644/1644 [==============================] - 1s 819us/step - loss: 16.2669 - root_mean_squared_error: 4.0303 - val_loss: 15.3847 - val_root_mean_squared_error: 3.9223\n",
      "Epoch 11/20\n",
      "1644/1644 [==============================] - 1s 722us/step - loss: 16.0697 - root_mean_squared_error: 4.0057 - val_loss: 15.2505 - val_root_mean_squared_error: 3.9052\n",
      "Epoch 12/20\n",
      "1644/1644 [==============================] - 1s 751us/step - loss: 15.9003 - root_mean_squared_error: 3.9844 - val_loss: 15.1349 - val_root_mean_squared_error: 3.8904\n",
      "Epoch 13/20\n",
      "1644/1644 [==============================] - 1s 691us/step - loss: 15.7523 - root_mean_squared_error: 3.9657 - val_loss: 15.0333 - val_root_mean_squared_error: 3.8773\n",
      "Epoch 14/20\n",
      "1644/1644 [==============================] - 1s 678us/step - loss: 15.6209 - root_mean_squared_error: 3.9490 - val_loss: 14.9423 - val_root_mean_squared_error: 3.8655\n",
      "Epoch 15/20\n",
      "1644/1644 [==============================] - 1s 726us/step - loss: 15.5028 - root_mean_squared_error: 3.9340 - val_loss: 14.8597 - val_root_mean_squared_error: 3.8548\n",
      "\n",
      "- Performance on *TRAINING* data:\n",
      "R2 = 0.96, RMSE = 3.88, MAE = 2.99, MAPE = 10.38%\n",
      "\n",
      "- Performance on *VALIDATION* data:\n",
      "R2 = 0.96, RMSE = 3.76, MAE = 2.88, MAPE = 9.46%\n",
      "\n",
      "- Performance on *TEST* data:\n",
      "R2 = 0.96, RMSE = 3.77, MAE = 2.9, MAPE = 9.3%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R2': 0.96, 'RMSE': 3.77, 'MAE': 2.9, 'MAPE': '9.3%'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the Model\n",
    "model = buildLinearModel(isBinary = False, label_width = 3)\n",
    "\n",
    "# Configure early stopping so we don't learn the training data too well at the expense of test/validation data (overfit)\n",
    "esCallback = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', mode=\"min\", patience=5, min_delta = 0.1, restore_best_weights = True)\n",
    "\n",
    "# NOTE: provide the Validation Dataset so that the Model does not check itself on Training Data\n",
    "model.fit(window.train, validation_data = window.val, callbacks = [esCallback], epochs = 20)\n",
    "\n",
    "print(\"\\r\\n- Performance on *TRAINING* data:\")\n",
    "evaluateRegressionModel(model, window.train)\n",
    "print(\"\\r\\n- Performance on *VALIDATION* data:\")\n",
    "evaluateRegressionModel(model, window.val)\n",
    "print(\"\\r\\n- Performance on *TEST* data:\")\n",
    "evaluateRegressionModel(model, window.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Our Linear Model for predicting temperature 6 hours in advance is, on average, only `3F` off (RMSE is closer to `4F` as it is more sensitive to outliers by design).\n",
    "\n",
    "As a sanity check let's see how we do trying to predict Tempearature 24 hours in advance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1644/1644 [==============================] - 1s 727us/step - loss: 651.4777 - root_mean_squared_error: 17.3271 - val_loss: 62.9535 - val_root_mean_squared_error: 7.9343\n",
      "Epoch 2/20\n",
      "1644/1644 [==============================] - 1s 698us/step - loss: 62.7116 - root_mean_squared_error: 7.9154 - val_loss: 58.8930 - val_root_mean_squared_error: 7.6742\n",
      "Epoch 3/20\n",
      "1644/1644 [==============================] - 1s 713us/step - loss: 59.7334 - root_mean_squared_error: 7.7249 - val_loss: 57.0960 - val_root_mean_squared_error: 7.5562\n",
      "Epoch 4/20\n",
      "1644/1644 [==============================] - 1s 712us/step - loss: 57.7763 - root_mean_squared_error: 7.5974 - val_loss: 55.6181 - val_root_mean_squared_error: 7.4578\n",
      "Epoch 5/20\n",
      "1644/1644 [==============================] - 1s 732us/step - loss: 56.1512 - root_mean_squared_error: 7.4898 - val_loss: 54.4145 - val_root_mean_squared_error: 7.3766\n",
      "Epoch 6/20\n",
      "1644/1644 [==============================] - 1s 743us/step - loss: 54.8301 - root_mean_squared_error: 7.4011 - val_loss: 53.4288 - val_root_mean_squared_error: 7.3095\n",
      "Epoch 7/20\n",
      "1644/1644 [==============================] - 1s 743us/step - loss: 53.7344 - root_mean_squared_error: 7.3268 - val_loss: 52.6021 - val_root_mean_squared_error: 7.2527\n",
      "Epoch 8/20\n",
      "1644/1644 [==============================] - 1s 787us/step - loss: 52.8027 - root_mean_squared_error: 7.2631 - val_loss: 51.8944 - val_root_mean_squared_error: 7.2038\n",
      "Epoch 9/20\n",
      "1644/1644 [==============================] - 1s 835us/step - loss: 51.9954 - root_mean_squared_error: 7.2074 - val_loss: 51.2798 - val_root_mean_squared_error: 7.1610\n",
      "Epoch 10/20\n",
      "1644/1644 [==============================] - 1s 817us/step - loss: 51.2870 - root_mean_squared_error: 7.1581 - val_loss: 50.7414 - val_root_mean_squared_error: 7.1233\n",
      "Epoch 11/20\n",
      "1644/1644 [==============================] - 1s 849us/step - loss: 50.6604 - root_mean_squared_error: 7.1143 - val_loss: 50.2669 - val_root_mean_squared_error: 7.0899\n",
      "Epoch 12/20\n",
      "1644/1644 [==============================] - 1s 851us/step - loss: 50.1033 - root_mean_squared_error: 7.0751 - val_loss: 49.8472 - val_root_mean_squared_error: 7.0603\n",
      "Epoch 13/20\n",
      "1644/1644 [==============================] - 1s 778us/step - loss: 49.6064 - root_mean_squared_error: 7.0399 - val_loss: 49.4750 - val_root_mean_squared_error: 7.0338\n",
      "Epoch 14/20\n",
      "1644/1644 [==============================] - 1s 862us/step - loss: 49.1620 - root_mean_squared_error: 7.0083 - val_loss: 49.1442 - val_root_mean_squared_error: 7.0103\n",
      "Epoch 15/20\n",
      "1644/1644 [==============================] - 1s 787us/step - loss: 48.7640 - root_mean_squared_error: 6.9799 - val_loss: 48.8497 - val_root_mean_squared_error: 6.9893\n",
      "Epoch 16/20\n",
      "1644/1644 [==============================] - 1s 766us/step - loss: 48.4068 - root_mean_squared_error: 6.9543 - val_loss: 48.5871 - val_root_mean_squared_error: 6.9704\n",
      "Epoch 17/20\n",
      "1644/1644 [==============================] - 1s 801us/step - loss: 48.0858 - root_mean_squared_error: 6.9312 - val_loss: 48.3525 - val_root_mean_squared_error: 6.9536\n",
      "Epoch 18/20\n",
      "1644/1644 [==============================] - 1s 765us/step - loss: 47.7968 - root_mean_squared_error: 6.9104 - val_loss: 48.1426 - val_root_mean_squared_error: 6.9385\n",
      "Epoch 19/20\n",
      "1644/1644 [==============================] - 1s 784us/step - loss: 47.5360 - root_mean_squared_error: 6.8916 - val_loss: 47.9542 - val_root_mean_squared_error: 6.9249\n",
      "\n",
      "- Performance on *TRAINING* data:\n",
      "R2 = 0.89, RMSE = 6.89, MAE = 5.27, MAPE = 26.75%\n",
      "\n",
      "- Performance on *VALIDATION* data:\n",
      "R2 = 0.88, RMSE = 6.91, MAE = 5.27, MAPE = 22.59%\n",
      "\n",
      "- Performance on *TEST* data:\n",
      "R2 = 0.87, RMSE = 7.23, MAE = 5.45, MAPE = 21.87%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R2': 0.87, 'RMSE': 7.23, 'MAE': 5.45, 'MAPE': '21.87%'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = WindowGenerator(\n",
    "    input_width = 2, # Take 2h of history into account\n",
    "    label_width = 3,  # Corresponds to aggreagation half-interval of 1h\n",
    "    shift = 23, # Forecast 6 hours in Advance (24 - (AHI=1) = 23)\n",
    "    label_columns=['Temp'],\n",
    "    train_df = train_df, val_df = val_df, test_df = test_df # Create Tensorflow datasets for all 3 subsets\n",
    ")\n",
    "\n",
    "# Build the Model\n",
    "model = buildLinearModel(isBinary = False, label_width = 3)\n",
    "\n",
    "# Configure early stopping so we don't learn the training data too well at the expense of test/validation data (overfit)\n",
    "esCallback = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', mode=\"min\", patience=5, min_delta = 0.1, restore_best_weights = True)\n",
    "\n",
    "# NOTE: provide the Validation Dataset so that the Model does not check itself on Training Data\n",
    "model.fit(window.train, validation_data = window.val, callbacks = [esCallback], epochs = 20)\n",
    "\n",
    "print(\"\\r\\n- Performance on *TRAINING* data:\")\n",
    "evaluateRegressionModel(model, window.train)\n",
    "print(\"\\r\\n- Performance on *VALIDATION* data:\")\n",
    "evaluateRegressionModel(model, window.val)\n",
    "print(\"\\r\\n- Performance on *TEST* data:\")\n",
    "evaluateRegressionModel(model, window.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, once we increased prediction time by 18h the quality of our prediction went down. Our average error grew from under `3F` to over `5F`. Still, the forecast is quite good as errors of this magnitude allow us to know how to dress (more or less)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to other, more interesting, models let's try to fit a Binary Classification problem. Specifically, we'll fit `Precipitation` 12 hours in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1644/1644 [==============================] - 2s 904us/step - loss: 0.5485 - recall: 0.4581 - precision_12: 0.2761 - val_loss: 0.3535 - val_recall: 0.3887 - val_precision_12: 0.5149\n",
      "Epoch 2/20\n",
      "1644/1644 [==============================] - 1s 878us/step - loss: 0.3414 - recall: 0.3918 - precision_12: 0.5367 - val_loss: 0.2910 - val_recall: 0.2994 - val_precision_12: 0.5923\n",
      "Epoch 3/20\n",
      "1644/1644 [==============================] - 1s 879us/step - loss: 0.2897 - recall: 0.3294 - precision_12: 0.6083 - val_loss: 0.2727 - val_recall: 0.2874 - val_precision_12: 0.6317\n",
      "Epoch 4/20\n",
      "1644/1644 [==============================] - 1s 805us/step - loss: 0.2737 - recall: 0.3230 - precision_12: 0.6366 - val_loss: 0.2664 - val_recall: 0.2917 - val_precision_12: 0.6404\n",
      "Epoch 5/20\n",
      "1644/1644 [==============================] - 1s 861us/step - loss: 0.2680 - recall: 0.3270 - precision_12: 0.6448 - val_loss: 0.2640 - val_recall: 0.2971 - val_precision_12: 0.6397\n",
      "Epoch 6/20\n",
      "1644/1644 [==============================] - 1s 907us/step - loss: 0.2656 - recall: 0.3300 - precision_12: 0.6457 - val_loss: 0.2630 - val_recall: 0.3008 - val_precision_12: 0.6406\n",
      "\n",
      "- Performance on *TRAINING* data:\n",
      "1644/1644 [==============================] - 1s 400us/step\n",
      "[[37647  1730]\n",
      " [ 8144  5072]]\n",
      "Recall = 0.38, Precision = 0.74, F1 = 0.5, MCC = 0.43\n",
      "\n",
      "- Performance on *VALIDATION* data:\n",
      "548/548 [==============================] - 0s 384us/step\n",
      "[[12547   473]\n",
      " [ 2816  1688]]\n",
      "Recall = 0.37, Precision = 0.78, F1 = 0.5, MCC = 0.44\n",
      "\n",
      "- Performance on *TEST* data:\n",
      "548/548 [==============================] - 0s 387us/step\n",
      "[[12382   516]\n",
      " [ 2920  1707]]\n",
      "Recall = 0.36, Precision = 0.76, F1 = 0.49, MCC = 0.43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Recall': 0.36, 'Precision': 0.76, 'F1:': 0.49, 'MCC:': 0.43}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_use = ['_day_sin', '_day_cos', '_hour_sin', '_hour_cos', 'DewPoint', 'WindSpeed', '_cloud_intensity']\n",
    "featureset = buildFeatureSet(\n",
    "    '../processed-data/noaa_2011-2020_chicago_PREPROC.csv',\n",
    "    ['../processed-data/noaa_2011-2020_cedar-rapids_PREPROC.csv', \n",
    "     '../processed-data/noaa_2011-2020_madison_PREPROC.csv',\n",
    "     '../processed-data/noaa_2011-2020_des-moines_PREPROC.csv',\n",
    "    '../processed-data/noaa_2011-2020_rochester_PREPROC.csv'],\n",
    "    predictedVariable='_is_precip',\n",
    "    featuresToUse = features_to_use\n",
    "    )\n",
    "\n",
    "# Split\n",
    "n = len(featureset)\n",
    "train_df = featureset[0 : int(n*0.60)]\n",
    "val_df = featureset[int(n*0.60) : int(n*0.80)]\n",
    "test_df = featureset[int(n*0.80) : ]\n",
    "\n",
    "# Normalize\n",
    "train_df, val_df, test_df = normalizeData(train_df, val_df, test_df, \n",
    "                                          '_is_precip', features_to_use, 4)\n",
    "\n",
    "window = WindowGenerator(\n",
    "    input_width = 2, # Take 2h of history into account\n",
    "    label_width = 7,  # Corresponds to aggreagation half-interval of 3h\n",
    "    shift = 9, # Forecast 12 hours in Advance (12 - (AHI=3) = 9)\n",
    "    label_columns=['_is_precip'],\n",
    "    train_df = train_df, val_df = val_df, test_df = test_df # Create Tensorflow datasets for all 3 subsets\n",
    ")\n",
    "\n",
    "# Build the Model\n",
    "model = buildLinearModel(isBinary = True, label_width = 7)\n",
    "\n",
    "# Configure early stopping so we don't learn the training data too well at the expense of test/validation data (overfit)\n",
    "esCallback = tf.keras.callbacks.EarlyStopping(monitor='val_recall', mode=\"max\", patience=5, min_delta = 0.05, restore_best_weights = True)\n",
    "\n",
    "# NOTE: provide the Validation Dataset so that the Model does not check itself on Training Data\n",
    "model.fit(window.train, validation_data = window.val, callbacks = [esCallback], epochs = 20)\n",
    "\n",
    "print(\"\\r\\n- Performance on *TRAINING* data:\")\n",
    "evaluateClassificationModel(model, window.train)\n",
    "print(\"\\r\\n- Performance on *VALIDATION* data:\")\n",
    "evaluateClassificationModel(model, window.val)\n",
    "print(\"\\r\\n- Performance on *TEST* data:\")\n",
    "evaluateClassificationModel(model, window.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `val_recall` starts going up after the 3d iteration while the `recall` (computed on the Training Set) keeps going down. This reassures us that Early Stopping is an effective strategy in preventing Overfitting. \n",
    "\n",
    "Overall, this isn't a great model. We correctly catch just over a third of rainy / snowy periods. We may benefit from more complex models going forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) _Non-Linear Models_\n",
    "\n",
    "\n",
    "We shall explore the following models that are non-Linear in the hopes that they'll give us better results. These models shall be:\n",
    "\n",
    "- Single Hidden Layer Neural Network\n",
    "- Two Hidden Layer Neural Network\n",
    "- Convolutional Neural Network with a single Convolutional Layer\n",
    "\n",
    "(_NOTE: LSTM/RNN too was considered but through some preliminary experimentation I concluded that the **order** of datapoints does not noticeably affect the forecast. Since this type of network thrives on order being meanigful to the variable being learned, I did not deem RNN/LSTM worthy of inclusion due to their long training times_)\n",
    "\n",
    "The following functions construct each of these 3 additional kinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Hidden Layer\n",
    "def buildSimpleNNModel(isBinary, label_width):\n",
    "    _activation, _loss, _metrics  = getActivationLossAndMetrics(isBinary)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        # Use all time steps\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        tf.keras.layers.Dense(units=400, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=label_width, activation = _activation, kernel_initializer=tf.initializers.zeros()),\n",
    "\n",
    "        # Add back the time dimension.\n",
    "        # Shape: (outputs) => (1, outputs)\n",
    "        tf.keras.layers.Reshape([label_width, 1]),\n",
    "    ])\n",
    "    model.compile(loss=_loss, optimizer='adam', metrics = _metrics)\n",
    "    return model\n",
    "\n",
    "# Two Hidden Layers\n",
    "def buildDNNModel(isBinary, label_width):\n",
    "    _activation, _loss, _metrics = getActivationLossAndMetrics(isBinary)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "         # Use all time steps\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        tf.keras.layers.Dense(units=400, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=200, activation='relu'),\n",
    "\n",
    "        tf.keras.layers.Dense(units=label_width, activation = _activation, kernel_initializer=tf.initializers.zeros()),\n",
    "\n",
    "        # Add back the time dimension.\n",
    "        # Shape: (outputs) => (1, outputs)\n",
    "        tf.keras.layers.Reshape([label_width, 1]),\n",
    "    ])\n",
    "    model.compile(loss=_loss, optimizer='adam', metrics = _metrics)\n",
    "    return model\n",
    "\n",
    "# Convolutional\n",
    "def buildConvModel(isBinary, lookbackHours, label_width):\n",
    "    _activation, _loss, _metrics = getActivationLossAndMetrics(isBinary)\n",
    "    CONV_WIDTH = 4\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "\n",
    "        tf.keras.layers.Conv1D(filters=200,\n",
    "                               kernel_size=(CONV_WIDTH),\n",
    "                               activation='relu'),\n",
    "        #tf.keras.layers.Dense(units=100, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=label_width, activation=_activation, kernel_initializer=tf.initializers.zeros()),\n",
    "        tf.keras.layers.Reshape([label_width, 1]),\n",
    "        ])\n",
    "\n",
    "    model.compile(loss=_loss, optimizer='adam', metrics = [_metrics])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now repeat the above experiment (predict `Precipitation` 12 hours in advance) using the second of the additional networks above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1644/1644 [==============================] - 3s 1ms/step - loss: 0.3062 - recall: 0.2655 - precision_12: 0.6130 - val_loss: 0.2657 - val_recall: 0.2948 - val_precision_12: 0.6126\n",
      "Epoch 2/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2714 - recall: 0.3388 - precision_12: 0.6354 - val_loss: 0.2660 - val_recall: 0.3258 - val_precision_12: 0.6131\n",
      "Epoch 3/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2675 - recall: 0.3430 - precision_12: 0.6441 - val_loss: 0.2655 - val_recall: 0.3213 - val_precision_12: 0.6168\n",
      "Epoch 4/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2653 - recall: 0.3453 - precision_12: 0.6518 - val_loss: 0.2639 - val_recall: 0.3198 - val_precision_12: 0.6216\n",
      "Epoch 5/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2633 - recall: 0.3492 - precision_12: 0.6565 - val_loss: 0.2659 - val_recall: 0.3145 - val_precision_12: 0.6219\n",
      "Epoch 6/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2618 - recall: 0.3512 - precision_12: 0.6616 - val_loss: 0.2654 - val_recall: 0.2991 - val_precision_12: 0.6259\n",
      "Epoch 7/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2602 - recall: 0.3533 - precision_12: 0.6647 - val_loss: 0.2664 - val_recall: 0.3000 - val_precision_12: 0.6297\n",
      "Epoch 8/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2586 - recall: 0.3505 - precision_12: 0.6699 - val_loss: 0.2695 - val_recall: 0.3043 - val_precision_12: 0.6238\n",
      "Epoch 9/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2569 - recall: 0.3560 - precision_12: 0.6750 - val_loss: 0.2717 - val_recall: 0.3054 - val_precision_12: 0.6114\n",
      "Epoch 10/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2545 - recall: 0.3658 - precision_12: 0.6832 - val_loss: 0.2722 - val_recall: 0.3097 - val_precision_12: 0.5986\n",
      "Epoch 11/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2518 - recall: 0.3759 - precision_12: 0.6837 - val_loss: 0.2744 - val_recall: 0.3253 - val_precision_12: 0.5816\n",
      "Epoch 12/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2484 - recall: 0.3849 - precision_12: 0.6894 - val_loss: 0.2813 - val_recall: 0.3345 - val_precision_12: 0.5702\n",
      "Epoch 13/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2448 - recall: 0.3991 - precision_12: 0.6938 - val_loss: 0.2826 - val_recall: 0.3396 - val_precision_12: 0.5642\n",
      "Epoch 14/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2417 - recall: 0.4169 - precision_12: 0.6974 - val_loss: 0.2872 - val_recall: 0.3511 - val_precision_12: 0.5537\n",
      "Epoch 15/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2366 - recall: 0.4346 - precision_12: 0.7073 - val_loss: 0.2903 - val_recall: 0.3481 - val_precision_12: 0.5493\n",
      "Epoch 16/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2313 - recall: 0.4535 - precision_12: 0.7160 - val_loss: 0.2945 - val_recall: 0.3429 - val_precision_12: 0.5512\n",
      "Epoch 17/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2270 - recall: 0.4671 - precision_12: 0.7222 - val_loss: 0.3030 - val_recall: 0.3227 - val_precision_12: 0.5448\n",
      "Epoch 18/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2222 - recall: 0.4888 - precision_12: 0.7309 - val_loss: 0.3151 - val_recall: 0.3250 - val_precision_12: 0.5392\n",
      "Epoch 19/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2160 - recall: 0.5044 - precision_12: 0.7388 - val_loss: 0.3218 - val_recall: 0.3119 - val_precision_12: 0.5394\n",
      "Epoch 20/20\n",
      "1644/1644 [==============================] - 2s 1ms/step - loss: 0.2119 - recall: 0.5196 - precision_12: 0.7423 - val_loss: 0.3304 - val_recall: 0.3274 - val_precision_12: 0.5064\n",
      "\n",
      "- Performance on *TRAINING* data:\n",
      "1644/1644 [==============================] - 1s 546us/step\n",
      "[[37959  1418]\n",
      " [ 7058  6158]]\n",
      "Recall = 0.46, Precision = 0.81, F1 = 0.59, MCC = 0.53\n",
      "\n",
      "- Performance on *VALIDATION* data:\n",
      "548/548 [==============================] - 0s 548us/step\n",
      "[[12280   740]\n",
      " [ 2656  1848]]\n",
      "Recall = 0.41, Precision = 0.71, F1 = 0.52, MCC = 0.43\n",
      "\n",
      "- Performance on *TEST* data:\n",
      "548/548 [==============================] - 0s 548us/step\n",
      "[[12155   743]\n",
      " [ 2746  1881]]\n",
      "Recall = 0.4, Precision = 0.71, F1 = 0.51, MCC = 0.43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Recall': 0.4, 'Precision': 0.71, 'F1:': 0.51, 'MCC:': 0.43}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the Model\n",
    "model = buildDNNModel(isBinary = True, label_width = 7)\n",
    "\n",
    "# Configure early stopping so we don't learn the training data too well at the expense of test/validation data (overfit)\n",
    "esCallback = tf.keras.callbacks.EarlyStopping(monitor='val_recall', mode=\"max\", patience=10, min_delta = 0.001, restore_best_weights = True)\n",
    "\n",
    "# NOTE: provide the Validation Dataset so that the Model does not check itself on Training Data\n",
    "model.fit(window.train, validation_data = window.val, callbacks = [esCallback], epochs = 20)\n",
    "\n",
    "print(\"\\r\\n- Performance on *TRAINING* data:\")\n",
    "evaluateClassificationModel(model, window.train)\n",
    "print(\"\\r\\n- Performance on *VALIDATION* data:\")\n",
    "evaluateClassificationModel(model, window.val)\n",
    "print(\"\\r\\n- Performance on *TEST* data:\")\n",
    "evaluateClassificationModel(model, window.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is noticeably better than the Linear model we tried for the same problem just above! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Conclusion: Compare and Choose the Models\n",
    "\n",
    "We have presented four kinds of different models. The following script automatically builds and evaluates all of those models for all predicted variables and for all prediction intervals: https://github.com/sergeypine/WeatherLearner/blob/capstone-model-experiments/model-prototyping/NOAA_learner_benchmark2.py\n",
    "\n",
    "We shall not go over it here as it repackages the same code that was already presented above. The whole execution takes over an hour so reproducing that here wouldn't be efficient either. In total, 64 executions are performed (`4 models x 4 predicted quantities x 4 prediction intervals`)\n",
    "\n",
    "The tables below summarize the results based on the target metric discussed above. The \"winners\" are **highlighted**. Note that if two models give roughly the same results, the simpler one is chosen.\n",
    "\n",
    "(Note that complete results of model evaluation that include other metrics can be found here: https://github.com/sergeypine/WeatherLearner/blob/capstone-model-experiments/model-prototyping/tf_benchmark2.txt)\n",
    "\n",
    "NOTE: The above benchmarking script, instead of using all features and all locations uses only those features and locations that have been proven to benefit the prediction. For each of the 16 prediction targets, such optimal features and locations turned out to be different. See https://github.com/sergeypine/WeatherLearner/blob/capstone-model-experiments/model-prototyping/NOAA_feature_selector.py (this feature selection script tries features and locations one by one and evaluates whether such additions benefit performance) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarity Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Prediction Interval | LINEAR | NN | DNN | CNN |\n",
    "----------------------|--------|----|-----|-----|\n",
    "| 6h | **.96** | .96 | .96 | .96 |\n",
    "| 12h | **.81** | . 73 | . 77 | .71 |\n",
    "| 18h | .74 | **. 81** | .73 | .77 |\n",
    "| 24h | .64 | **.78** | . 64 | .76 |\n",
    "\n",
    "Interestingly, for shorter time intervals `LINEAR` is the most optimal but a simple `Single Layer Neural Network` does better for the longer intervals.\n",
    "\n",
    "We can see that for some models Recall actually increases as the forecast time increases, which at the first glance makes no sense. However, remember that we have optimized training specifically for `Recall`; such increases in `Recall` are more than compensated for by a drop in `Precision` (see full benchmarking results), resulting in far weaker models overall for longer forecast times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Prediction Interval |  LINEAR | NN | DNN | CNN |\n",
    "|--------------------|---------|----|-----|-----|\n",
    "| 6h | **.81** | .81 | .81 | .81 |\n",
    "| 12h | .44 | .58 | .50 | **.59** |\n",
    "| 18h | .37 | .42 | .42 | **.53** |\n",
    "| 24h | .26 | .45 | .29| **.46** | \n",
    "\n",
    "`Convolutional Neural Network` consistently gives the best results, except for the short term 6h forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature RMSE (degrees Farenheit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Prediction Interval |  LINEAR | NN | DNN | CNN |\n",
    "|--------------------|---------|----|-----|-----|\n",
    "| 6h | 3.39 | 3.03 | **2.97** | 3.04 |\n",
    "| 12h | 5.61 | **4.61** | 4.96 | 4.81 | \n",
    "| 18h | 6.11 | 6.39 | **5.93** | 6.24 | \n",
    "| 24h | **6.71** | 8.83 | 9.65 | 9.41 |\n",
    "\n",
    "Different types of `Neural Networks` work best for different intervals, except for the longest term 24h forecast. This is a rather puzzling conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wind Speed RMSE (miles per hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Prediction Interval |  LINEAR | NN | DNN | CNN |\n",
    "|--------------------|---------|----|-----|-----|\n",
    "| 6h | 1.97 | **1.89** | 1.95 | 1.91 |\n",
    "| 12h | 3.37 | **3.14** | 3.21 | 3.25 |\n",
    "| 18h | 3.85 | **3.75** | 3.95 | 3.81 |\n",
    "| 24h | **4.13** | 4.32 | 4.52 | 4.39 |\n",
    "\n",
    "`Simple Neural Network` gives superior results except for the shortest term 6h forecast."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
