{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago Weather Forecasting: Model Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I shall explain how one can build models for weather forecasting and offer evalutation of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Scope Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed with modelling, we need to define what the scope of this project is.\n",
    "\n",
    "### _(1) What we are forecasting_\n",
    "\n",
    "The goal is to predict a number of weather characteristics in the future for the city of Chicago. We shall select the basic componenets of a weather situation that a layperson will be considering for planning purposes. Think about deciding whether to go on a Sunday picnic on the beach or go hiking.  \n",
    "\n",
    "With that in mind, the following quantities are chosen:\n",
    "\n",
    "- Temperature (in Farenheit degrees)\n",
    "- Wind Speed (in miles per hour) \n",
    "- Precipitation (whether or not there will be rain / snow or hail)\n",
    "- Cloudiness (whether or not the sky will be covered in clouds)\n",
    "\n",
    "Of those, forecasting Temperature and Wind are **Regression** problems, while forecasting Precipitation or Cloudiness are **Binary Classification** problems.\n",
    "\n",
    "We shall attempt to predict weather for the following durations in advance:\n",
    "\n",
    "- 6 hours\n",
    "- 12 hours\n",
    "- 18 hours\n",
    "- 24 hours\n",
    "\n",
    "(Preliminary experimentation has proven longer term forecasts to be not feasible with the data available)\n",
    "\n",
    "### _(2)  Model Dataset_\n",
    "\n",
    "As explained previously, we shall be relying on US Government (NOAA) datasets containing **hourly** weather reports for the weather station in Chicago as well as nearby stations in the US Midwest. \n",
    "\n",
    "In this notebook we shall work with data for 10 years, 2011-2020 (inclusively) from the following locations:\n",
    "\n",
    "- Chicago, IL (target location)\n",
    "- Cedar Rapids, IA\n",
    "- Des Moines, IA\n",
    "- Rochester, MN\n",
    "- Quincy, IL\n",
    "- Madison, WI\n",
    "- St Louis, MO\n",
    "- Green Bay, WI\n",
    "- Lansing, MI\n",
    "\n",
    "Most of these locations are **West** of Chicago as we previously determined through correlation analysis that locations in that direction have much more effect on weather in Chicago than locations in other directions.\n",
    "\n",
    "Finally, we shall be using preprocessed reports rather than the non-intuitive raw NOAA reports. See the following  (Timestamp is followed by the 4 quantities we aim to forecast as well as a few more for demo purposes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>Temp</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>_is_precip</th>\n",
       "      <th>_is_cloudy</th>\n",
       "      <th>CloudCondition</th>\n",
       "      <th>WeatherType</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>_wind_dir_sin</th>\n",
       "      <th>_wind_dir_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>40.333333</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.720</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-3.420201e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.735</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-1.736482e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.750</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.750</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PartlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.760</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011-01-01 05:00:00</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MostlyClear</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.770</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011-01-01 06:00:00</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.785</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2011-01-01 07:00:00</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.810</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2011-01-01 08:00:00</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.870</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2011-01-01 09:00:00</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.890</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-3.420201e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2011-01-01 10:00:00</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.910</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011-01-01 11:00:00</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.890</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2011-01-01 12:00:00</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.890</td>\n",
       "      <td>55.500000</td>\n",
       "      <td>-0.906308</td>\n",
       "      <td>-4.226183e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2011-01-01 13:00:00</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.900</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-1.736482e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2011-01-01 14:00:00</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.930</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2011-01-01 15:00:00</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.960</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-1.736482e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011-01-01 16:00:00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>29.990</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2011-01-01 17:00:00</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>LightSnow</td>\n",
       "      <td>30.030</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-3.420201e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2011-01-01 18:00:00</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>30.040</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2011-01-01 19:00:00</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PartlyCloudy</td>\n",
       "      <td>NoPrecipitation</td>\n",
       "      <td>30.070</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-1.736482e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   DATE       Temp  WindSpeed  _is_precip  _is_cloudy  \\\n",
       "0   2011-01-01 00:00:00  40.333333       13.0           0           1   \n",
       "1   2011-01-01 01:00:00  37.000000       17.0           0           1   \n",
       "2   2011-01-01 02:00:00  36.000000       17.0           0           1   \n",
       "3   2011-01-01 03:00:00  32.000000       15.0           0           1   \n",
       "4   2011-01-01 04:00:00  31.000000       16.0           0           0   \n",
       "5   2011-01-01 05:00:00  28.000000       18.0           0           0   \n",
       "6   2011-01-01 06:00:00  27.500000       17.0           0           1   \n",
       "7   2011-01-01 07:00:00  25.000000       20.0           0           1   \n",
       "8   2011-01-01 08:00:00  23.000000       21.0           0           1   \n",
       "9   2011-01-01 09:00:00  21.000000       23.0           0           1   \n",
       "10  2011-01-01 10:00:00  20.000000       21.0           0           1   \n",
       "11  2011-01-01 11:00:00  19.000000       24.0           0           1   \n",
       "12  2011-01-01 12:00:00  20.000000       23.0           0           1   \n",
       "13  2011-01-01 13:00:00  19.000000       25.0           0           1   \n",
       "14  2011-01-01 14:00:00  19.000000       18.0           0           1   \n",
       "15  2011-01-01 15:00:00  19.000000       20.0           0           1   \n",
       "16  2011-01-01 16:00:00  18.000000       16.0           0           1   \n",
       "17  2011-01-01 17:00:00  16.000000       20.0           1           1   \n",
       "18  2011-01-01 18:00:00  16.000000       18.0           0           1   \n",
       "19  2011-01-01 19:00:00  15.000000       18.0           0           0   \n",
       "\n",
       "   CloudCondition      WeatherType  Pressure   Humidity  _wind_dir_sin  \\\n",
       "0          Cloudy  NoPrecipitation    29.720  71.333333      -0.939693   \n",
       "1          Cloudy  NoPrecipitation    29.735  70.000000      -0.984808   \n",
       "2          Cloudy  NoPrecipitation    29.750  70.000000      -0.866025   \n",
       "3    MostlyCloudy  NoPrecipitation    29.750  61.000000      -0.866025   \n",
       "4    PartlyCloudy  NoPrecipitation    29.760  61.000000      -0.866025   \n",
       "5     MostlyClear  NoPrecipitation    29.770  63.000000      -0.866025   \n",
       "6    MostlyCloudy  NoPrecipitation    29.785  67.500000      -0.866025   \n",
       "7    MostlyCloudy  NoPrecipitation    29.810  75.000000      -0.866025   \n",
       "8          Cloudy  NoPrecipitation    29.870  65.000000      -0.866025   \n",
       "9          Cloudy  NoPrecipitation    29.890  62.000000      -0.939693   \n",
       "10         Cloudy  NoPrecipitation    29.910  55.000000      -0.866025   \n",
       "11   MostlyCloudy  NoPrecipitation    29.890  57.000000      -0.866025   \n",
       "12   MostlyCloudy  NoPrecipitation    29.890  55.500000      -0.906308   \n",
       "13   MostlyCloudy  NoPrecipitation    29.900  57.000000      -0.984808   \n",
       "14   MostlyCloudy  NoPrecipitation    29.930  54.000000      -0.866025   \n",
       "15   MostlyCloudy  NoPrecipitation    29.960  51.000000      -0.984808   \n",
       "16   MostlyCloudy  NoPrecipitation    29.990  62.000000      -1.000000   \n",
       "17   MostlyCloudy        LightSnow    30.030  67.000000      -0.939693   \n",
       "18   MostlyCloudy  NoPrecipitation    30.040  61.500000      -0.965926   \n",
       "19   PartlyCloudy  NoPrecipitation    30.070  59.000000      -0.984808   \n",
       "\n",
       "    _wind_dir_cos  \n",
       "0   -3.420201e-01  \n",
       "1   -1.736482e-01  \n",
       "2   -5.000000e-01  \n",
       "3   -5.000000e-01  \n",
       "4   -5.000000e-01  \n",
       "5   -5.000000e-01  \n",
       "6   -5.000000e-01  \n",
       "7   -5.000000e-01  \n",
       "8   -5.000000e-01  \n",
       "9   -3.420201e-01  \n",
       "10  -5.000000e-01  \n",
       "11  -5.000000e-01  \n",
       "12  -4.226183e-01  \n",
       "13  -1.736482e-01  \n",
       "14  -5.000000e-01  \n",
       "15  -1.736482e-01  \n",
       "16  -1.836970e-16  \n",
       "17  -3.420201e-01  \n",
       "18  -2.588190e-01  \n",
       "19  -1.736482e-01  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../processed-data/noaa_2011-2020_chicago_PREPROC.csv')\n",
    "subset_df = df [['DATE', 'Temp', 'WindSpeed', '_is_precip', '_is_cloudy', 'CloudCondition', 'WeatherType', \n",
    "                 'Pressure', 'Humidity', '_wind_dir_sin', '_wind_dir_cos']]\n",
    "subset_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _(3)  Aggregated Forecasting_\n",
    "\n",
    "As explained previously, the datasets are chronological lists of hourly data points. What does it mean to forecast each of the target quantities, say, 12h, in advance?\n",
    "\n",
    "Predicting weather for a particular hour may not serve us particularly well. Consider the following situations: \n",
    "\n",
    "- let's say it is 11PM and we are considering a picnic at 11AM the next day. If it does not rain at 11AM but it does rain at 10AM or 1PM, the picnic is a bad idea.\n",
    "\n",
    "- similarly, if we are considering kayaking, if the wind is going to be 5mph at 11AM but 30mph at 2PM, we should reconsider\n",
    "\n",
    "*To address such concerns, we shall be attempting to forecast not weather for the exact target hour but rather some kind of **aggregation over an interval** centered around that hour.*\n",
    "\n",
    "In the code to follow we shall rely on something called **Aggregation Half Interval (AHI)**. For example, if AHI = 3 and the target hour is 11AM, we shall be considering the interval spanning 08AM to 02PM. \n",
    "\n",
    "Let us now define what that means for each of the 4 forecasted quantities:\n",
    "\n",
    "| Quantity | AHI | Aggregation Rule |\n",
    "| --- | --- | --- |\n",
    "| Temperature | 1h | Average |\n",
    "| WindSpeed   | 2h | Average |\n",
    "| Precipitation | 3h | True if any element is True |\n",
    "| Cloudiness | 3h  | True if any element is True |\n",
    "\n",
    "The first two rows for analog quantities are self explanatory: we are smoothing the prediction over an interval by averaging. Temperature has a smaller interval as it is much more directly dependent on time of day than wind.\n",
    "\n",
    "The last two rows for binary quantities say that if *any* hour during the interval is Rainy or Cloudy, the resulting forecast too is Rainy or Cloudy. As per the situation described above, if it rains anywhere close to the hour for which we are forecasting, we'll get wet. Similarly, if it is cloudy anywhere close to that hour, our sun tanning won't go well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Learning Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) _Merge Data from All Locations_\n",
    "\n",
    "We need to do a JOIN on all the weather reports whose data we'll be feeding into our models. The following functions perform the merge and drop any irrelevant columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFeatureSet(targetLocationFile, adjacentLocationFiles, predictedVariable, featuresToUse):\n",
    "    target_df = pd.read_csv(targetLocationFile, parse_dates=['DATE'])\n",
    "    target_df = dropUnusedColumns(target_df, predictedVariable, featuresToUse)\n",
    "    merged_df = target_df\n",
    "    suffix_no = 1\n",
    "\n",
    "    # Merge adjacent location files one by one relying on DATE\n",
    "    for adjacentLocationFile in adjacentLocationFiles:\n",
    "        adjacent_df = pd.read_csv(adjacentLocationFile, parse_dates=['DATE'])\n",
    "        adjacent_df = dropUnusedColumns(adjacent_df, predictedVariable, featuresToUse)\n",
    "\n",
    "        #Take control of column name suffix in the dataset being merged in\n",
    "        adjacent_df = adjacent_df.add_suffix(str(suffix_no))\n",
    "        adjacent_df = adjacent_df.rename(columns = {\"DATE{}\".format(suffix_no) :'DATE'})\n",
    "        merged_df = pd.merge(merged_df, adjacent_df, on='DATE')\n",
    "        suffix_no = suffix_no + 1\n",
    "\n",
    "    # DATE column is of no use in the modelling stage (we only needed it for merging)\n",
    "    merged_df = merged_df.drop(columns=['DATE'])\n",
    "    return merged_df\n",
    "\n",
    "#======================================================================\n",
    "# Keep only the DATE column, the variable we are predicting and the variables that we use for prediction\n",
    "def dropUnusedColumns(df, predictedVariable, featuresToUse):\n",
    "    all_columns = featuresToUse.copy()\n",
    "    all_columns.append('DATE')\n",
    "    all_columns.append(predictedVariable)\n",
    "    df = df[all_columns]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_wind_dir_sin</th>\n",
       "      <th>_wind_dir_cos</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>_wind_dir_sin1</th>\n",
       "      <th>_wind_dir_cos1</th>\n",
       "      <th>WindSpeed1</th>\n",
       "      <th>_wind_dir_sin2</th>\n",
       "      <th>_wind_dir_cos2</th>\n",
       "      <th>WindSpeed2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-0.342020</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.802123</td>\n",
       "      <td>-0.597159</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>23.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-0.173648</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-0.342020</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>-0.173648</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-0.342020</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>-0.939693</td>\n",
       "      <td>-0.342020</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _wind_dir_sin  _wind_dir_cos  WindSpeed  _wind_dir_sin1  _wind_dir_cos1  \\\n",
       "0      -0.939693      -0.342020       13.0       -0.802123       -0.597159   \n",
       "1      -0.984808      -0.173648       17.0       -0.642788       -0.766044   \n",
       "2      -0.866025      -0.500000       17.0       -0.766044       -0.642788   \n",
       "3      -0.866025      -0.500000       15.0       -0.866025       -0.500000   \n",
       "4      -0.866025      -0.500000       16.0       -0.866025       -0.500000   \n",
       "\n",
       "   WindSpeed1  _wind_dir_sin2  _wind_dir_cos2  WindSpeed2  \n",
       "0   23.666667       -0.866025       -0.500000        23.5  \n",
       "1   25.000000       -0.939693       -0.342020        24.0  \n",
       "2   23.000000       -0.984808       -0.173648        22.0  \n",
       "3   23.000000       -0.939693       -0.342020        22.0  \n",
       "4   23.000000       -0.939693       -0.342020        16.0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset = buildFeatureSet(\n",
    "    '../processed-data/noaa_2011-2020_chicago_PREPROC.csv',\n",
    "    ['../processed-data/noaa_2011-2020_cedar-rapids_PREPROC.csv', \n",
    "         '../processed-data/noaa_2011-2020_des-moines_PREPROC.csv'],\n",
    "    predictedVariable='WindSpeed',\n",
    "    featuresToUse = ['_wind_dir_sin', '_wind_dir_cos']\n",
    "    )\n",
    "featureset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of 3 variables of interest: `WindSpeed` (predicted) as well as `_wind_dir_sin` and `_wind_dir_cos` (to be used for predicting). As you can see, the dataset above has these variables repeated 3 times, once for each location. This merged kind of dataset will be used going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) _Split and Normalize the Data_\n",
    "\n",
    "Before we can train models we must split the data into the 3 subsets:\n",
    "\n",
    "- *Training*: the actual data that we'll be training on. This is the largest subset.\n",
    "- *Validation*: the dataset to be used for model tuning during training to check the model periodically\n",
    "- *Testing*: the dataset that will be hidden from the model training process and be used for final model evaluation\n",
    "\n",
    "Of course, we'll also need to normalize the features on which we are training to avoid algorithms issues like gradient explosion. The following code achieves both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_wind_dir_sin</th>\n",
       "      <th>_wind_dir_cos</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>_wind_dir_sin1</th>\n",
       "      <th>_wind_dir_cos1</th>\n",
       "      <th>WindSpeed1</th>\n",
       "      <th>_wind_dir_sin2</th>\n",
       "      <th>_wind_dir_cos2</th>\n",
       "      <th>WindSpeed2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.188696</td>\n",
       "      <td>-0.466556</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-1.144549</td>\n",
       "      <td>-0.762454</td>\n",
       "      <td>2.369524</td>\n",
       "      <td>-1.332160</td>\n",
       "      <td>-0.567817</td>\n",
       "      <td>2.530257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.256421</td>\n",
       "      <td>-0.236236</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.902651</td>\n",
       "      <td>-0.987495</td>\n",
       "      <td>2.595942</td>\n",
       "      <td>-1.448306</td>\n",
       "      <td>-0.362728</td>\n",
       "      <td>2.621978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.078108</td>\n",
       "      <td>-0.682661</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1.089776</td>\n",
       "      <td>-0.823255</td>\n",
       "      <td>2.256315</td>\n",
       "      <td>-1.519435</td>\n",
       "      <td>-0.144148</td>\n",
       "      <td>2.255092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.078108</td>\n",
       "      <td>-0.682661</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.241564</td>\n",
       "      <td>-0.632990</td>\n",
       "      <td>2.256315</td>\n",
       "      <td>-1.448306</td>\n",
       "      <td>-0.362728</td>\n",
       "      <td>2.255092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.078108</td>\n",
       "      <td>-0.682661</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-1.241564</td>\n",
       "      <td>-0.632990</td>\n",
       "      <td>2.256315</td>\n",
       "      <td>-1.448306</td>\n",
       "      <td>-0.362728</td>\n",
       "      <td>1.154431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _wind_dir_sin  _wind_dir_cos  WindSpeed  _wind_dir_sin1  _wind_dir_cos1  \\\n",
       "0      -1.188696      -0.466556       13.0       -1.144549       -0.762454   \n",
       "1      -1.256421      -0.236236       17.0       -0.902651       -0.987495   \n",
       "2      -1.078108      -0.682661       17.0       -1.089776       -0.823255   \n",
       "3      -1.078108      -0.682661       15.0       -1.241564       -0.632990   \n",
       "4      -1.078108      -0.682661       16.0       -1.241564       -0.632990   \n",
       "\n",
       "   WindSpeed1  _wind_dir_sin2  _wind_dir_cos2  WindSpeed2  \n",
       "0    2.369524       -1.332160       -0.567817    2.530257  \n",
       "1    2.595942       -1.448306       -0.362728    2.621978  \n",
       "2    2.256315       -1.519435       -0.144148    2.255092  \n",
       "3    2.256315       -1.448306       -0.362728    2.255092  \n",
       "4    2.256315       -1.448306       -0.362728    1.154431  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def normalizeData(trainDf, valDf,  testDf, predictedVariable, featuresToUse, adjacentLocationCount):\n",
    "\n",
    "    columns_to_normalize = featuresToUse.copy()\n",
    "\n",
    "    prefixes_to_normalize = featuresToUse.copy()\n",
    "    prefixes_to_normalize.append(predictedVariable)\n",
    "    for loc in range(1, 1 + adjacentLocationCount):\n",
    "        for prefix in prefixes_to_normalize:\n",
    "            columns_to_normalize.append(\"{}{}\".format(prefix, loc))\n",
    "\n",
    "    # Normalize input data but not the target variable\n",
    "    train_mean = trainDf[columns_to_normalize].mean()\n",
    "    train_std = trainDf[columns_to_normalize].std()\n",
    "\n",
    "    trainDf[columns_to_normalize] = (trainDf[columns_to_normalize] - train_mean) / train_std\n",
    "    valDf[columns_to_normalize] = (valDf[columns_to_normalize] - train_mean) / train_std\n",
    "    testDf[columns_to_normalize] = (testDf[columns_to_normalize] - train_mean) / train_std\n",
    "\n",
    "    return trainDf, valDf, testDf\n",
    "\n",
    "\n",
    "# Split the data: 6 years for training, 2 for validation & 2 for testing\n",
    "n = len(featureset)\n",
    "train_df = featureset[0 : int(n*0.60)]\n",
    "val_df = featureset[int(n*0.60) : int(n*0.80)]\n",
    "test_df = featureset[int(n*0.80) : ]\n",
    "\n",
    "# Normalize input data\n",
    "train_df, val_df, test_df = normalizeData(train_df, val_df, test_df, \n",
    "                                          'WindSpeed', ['_wind_dir_sin', '_wind_dir_cos'], 2)\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) _Prepare the Data for TensorFlow_\n",
    "\n",
    "We still have further to go before we can use TensorFlow to build models. \n",
    "\n",
    "First, we need to create a Sliding Window type data structure containing a number of observations in the Past. For example if we are forecasting _Temperature_ in 12h in advance and we want to look back 3 hours, we need `Temperature[-12h], Temperature[-13h], Temperature[-14h]` all in one row.\n",
    "\n",
    "Second, TensorFlow is quite particular about what form the input data should take:\n",
    "\n",
    "_Typically data in TensorFlow is packed into arrays where the outermost index is across examples (the \"batch\" dimension). The middle indices are the \"time\" or \"space\" (width, height) dimension(s). The innermost indices are the features_ (see https://www.tensorflow.org/tutorials/structured_data/time_series).\n",
    "\n",
    "The following class borrowed from the manual above takes the Pandas dataset and massages it into a Sliding Window tensor of the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "class WindowGenerator():\n",
    "\n",
    "    def __init__(self, \n",
    "        input_width, # Lookback Window (hours into the past to base predictions on)\n",
    "        label_width, # Aggregation Interval (how many hours of data we'll be predicting)\n",
    "        shift, # How many hours in advance we'll be predicting\n",
    "        train_df, val_df, test_df, # Training, Validation and Testing sets\n",
    "        label_columns=None):\n",
    "\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "          self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "          data=data,\n",
    "          targets=None,\n",
    "          sequence_length=self.total_window_size,\n",
    "          sequence_stride=1,\n",
    "          shuffle=False,\n",
    "          batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now use the above to demonstrate generation of Tensorflow Datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape (batch, time, features): (32, 12, 9)\n",
      "Labels shape (batch, time, features): (32, 5, 9)\n"
     ]
    }
   ],
   "source": [
    "wg = WindowGenerator(\n",
    "    input_width = 12, # Take 12h of history into account\n",
    "    label_width = 5,  # Corresponds to aggreagation half-interval of 2h\n",
    "    shift = 4, # Forecast 6 hours in Advance (6 - (AHI=2) = 4)\n",
    "    train_df = train_df, val_df = val_df, test_df = test_df # Create Tensorflow datasets for all 3 subsets\n",
    ")\n",
    "\n",
    "for example_inputs, example_labels in wg.train.take(1):\n",
    "    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "    print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those shapes are explained as follows:\n",
    "\n",
    "- 32 is the batch size (Tensorflow trains on data in batches)\n",
    "- 12 is the number of hours: for Inputs it is 12 because we are looking 12 hours back while for Outputs it is 5 because we are aggregating over 5h \n",
    "- 9 is the number of features: `WindSpeed`, `_wind_dir_sin` and `_wind_dir_cos` for each of the 3 locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) _Our First Model: Linear_\n",
    "\n",
    "It is time to build our very first model! It is the simplest one that there is, the `Linear` kind. The approach that we'll take is also borrowed from https://www.tensorflow.org/tutorials/structured_data/time_series . The goal is to predict *all* target labels in the aggregation interval in one shot. So in the case above, where the interval is 5, we'll be predicting 5 values in chronological order. \n",
    "\n",
    "We are going to make it work for both Binary Classification and Regression problems as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation and Loss are different for Binary Classification and Regression\n",
    "def getActivationAndLoss(isBinary):\n",
    "    activation, loss = \"linear\", 'mean_absolute_error'\n",
    "    if isBinary:\n",
    "        activation, loss = \"sigmoid\", \"binary_crossentropy\"\n",
    "    return activation, loss\n",
    "\n",
    "def buildLinearModel(isBinary, label_width):\n",
    "    _activation, _loss = getActivationAndLoss(isBinary)\n",
    "    model = tf.keras.Sequential([\n",
    "        # Take the last time-step.\n",
    "        # Shape [batch, time, features] => [batch, 1, features]\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "\n",
    "        tf.keras.layers.Dense(units=label_width, activation = _activation, kernel_initializer=tf.initializers.zeros()),\n",
    "        \n",
    "        # We shall be predicting a sequence of outputs rather than just one\n",
    "        tf.keras.layers.Reshape([label_width, 1]),\n",
    "    ])\n",
    "    model.compile(loss=_loss, optimizer='adam')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) _Model Evaluation_\n",
    "\n",
    "Before we proceed to actually train our model we need to establish the criteria based on which we'll evaluate it. \n",
    "\n",
    "This is a Regression model so we'll use the following metrics:\n",
    "\n",
    "- Root Mean Squared Error\n",
    "- Absolute Mean Error (less affected by outliers)\n",
    "- R2 Score\n",
    "- MAPE (Mean Absolute Percentage Error)\n",
    "\n",
    "While we are on the subject, we shall also define the Binary Classification Metrics (we'll use them later):\n",
    "\n",
    "- Recall\n",
    "- Precision\n",
    "- F1 Score\n",
    "- MCC (Matthew Coefficient, arguably the most balanced measure of Binary Classification performance)\n",
    "\n",
    "Finally, we'll need some extra code to evaluate the model. Remember, our models are predicting a sequence of values over an interval, whereas our application calls for aggregating values over that sequence. The code below will handle that as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import math\n",
    "\n",
    "# https://www.statology.org/mape-python/\n",
    "def calcMape(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred) \n",
    "    actual[actual == 0] = 0.1 # A meh hack to avoid division by 0\n",
    "\n",
    "    return np.mean(np.abs((actual - pred) / actual )) * 100 \n",
    "\n",
    "# https://kodify.net/python/math/truncate-decimals/\t\n",
    "def truncate(number, decimals=0):\n",
    "    \"\"\"\n",
    "    Returns a value truncated to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer.\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more.\")\n",
    "    elif decimals == 0:\n",
    "        return math.trunc(number)\n",
    "\n",
    "    factor = 10.0 ** decimals\n",
    "    return math.trunc(number * factor) / factor\n",
    "\n",
    "def evaluateClassificationModel(model, testSet):\n",
    "\n",
    "    predicted_labels =(model.predict(testSet, verbose = 1) > 0.5).astype(\"int32\")\n",
    "    true_labels = np.concatenate([y for x, y in testSet], axis=0)\n",
    "\n",
    "    assert len(predicted_labels) == len (true_labels)\n",
    "\n",
    "    # We are forecasting for a number of hours: aggregate each forecast series using the \"True iff 1 or more is True\" rule\n",
    "    predicted_agg = []\n",
    "    true_agg = []\n",
    "    for i in range(0, len(predicted_labels)):\n",
    "        predicted_i = predicted_labels[i].flatten()\n",
    "        true_i = true_labels[i].flatten()\n",
    "\n",
    "        predicted_i_agg = 1 if sum(predicted_i) > 0 else 0\n",
    "        true_i_agg = 1 if sum(true_i) > 0 else 0\n",
    "\n",
    "        predicted_agg.append(predicted_i_agg)\n",
    "        true_agg.append(true_i_agg)\n",
    "\n",
    "    recall = truncate(recall_score(true_agg, predicted_agg), 2)\n",
    "    precision = truncate(precision_score(true_agg, predicted_agg), 2)\n",
    "    f1 = truncate(f1_score(true_agg, predicted_agg), 2)\n",
    "    mcc = truncate(matthews_corrcoef(true_agg, predicted_agg), 2)\n",
    "\n",
    "    print(confusion_matrix(true_agg, predicted_agg))\n",
    "    print(\"Recall = {}, Precision = {}, F1 = {}, MCC = {}\".format(recall, precision, f1, mcc))\n",
    "\n",
    "    return {\n",
    "        \"Recall\": recall,\n",
    "        \"Precision\": precision, \n",
    "        \"F1:\" : f1,\n",
    "        \"MCC:\": mcc\n",
    "    }\t\n",
    "\n",
    "def evaluateRegressionModel(model, testSet):\n",
    "    predicted_values = model.predict(testSet)\n",
    "    true_values = np.concatenate([y for x, y in testSet], axis=0)\n",
    "\n",
    "    assert len(predicted_values) == len (true_values)\n",
    "\n",
    "    predicted_agg = []\n",
    "    true_agg = []\n",
    "    \n",
    "    # Averaging is our aggregation method\n",
    "    for i in range(0, len(predicted_values)):\n",
    "        predicted_i = predicted_values[i].flatten()\n",
    "        true_i = true_values[i].flatten()\n",
    "\n",
    "        predicted_i_agg, true_i_agg = np.mean(predicted_i), np.mean(true_i)\n",
    "        predicted_agg.append(predicted_i_agg)\n",
    "        true_agg.append(true_i_agg)\n",
    "\n",
    "    rmse = truncate(math.sqrt(mean_squared_error(true_agg, predicted_agg)), 2)\n",
    "    mae = truncate(mean_absolute_error(true_agg, predicted_agg), 2)\n",
    "    r2 = truncate(r2_score(true_agg, predicted_agg), 2)\n",
    "    mape = truncate(calcMape(true_agg, predicted_agg), 2) \n",
    "\n",
    "    print(\"R2 = {}, RMSE = {}, MAE = {}, MAPE = {}%\".format(r2, rmse, mae, mape))\n",
    "    return {\n",
    "        'R2' : r2,\n",
    "        'RMSE' : rmse,\n",
    "        'MAE' : mae,\n",
    "        'MAPE' : \"{}%\".format(mape)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) _Fit our First Model and Evaluate the Result_\n",
    "\n",
    "We are now ready to fit our first model to try and predict Temperature. \n",
    "\n",
    "Note that we'll configure Early Stopping to prevent us from Overfitting the Training Set as well as save on training time when the returns become too deminishing.\n",
    "\n",
    "First, though, build a less skimpy featureset than demoed above to get better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the Previous steps for a better dataset\n",
    "features_to_use = ['_day_sin', '_day_cos', '_hour_sin', '_hour_cos', 'DewPoint', 'WindSpeed', '_cloud_intensity']\n",
    "featureset = buildFeatureSet(\n",
    "    '../processed-data/noaa_2011-2020_chicago_PREPROC.csv',\n",
    "    ['../processed-data/noaa_2011-2020_cedar-rapids_PREPROC.csv', \n",
    "     '../processed-data/noaa_2011-2020_madison_PREPROC.csv',\n",
    "     '../processed-data/noaa_2011-2020_des-moines_PREPROC.csv',\n",
    "    '../processed-data/noaa_2011-2020_rochester_PREPROC.csv'],\n",
    "    predictedVariable='Temp',\n",
    "    featuresToUse = features_to_use\n",
    "    )\n",
    "n = len(featureset)\n",
    "train_df = featureset[0 : int(n*0.60)]\n",
    "val_df = featureset[int(n*0.60) : int(n*0.80)]\n",
    "test_df = featureset[int(n*0.80) : ]\n",
    "train_df, val_df, test_df = normalizeData(train_df, val_df, test_df, \n",
    "                                          'Temp', features_to_use, 4)\n",
    "wg = WindowGenerator(\n",
    "    input_width = 6, # Take 6h of history into account\n",
    "    label_width = 3,  # Corresponds to aggreagation half-interval of 1h\n",
    "    shift = 5, # Forecast 6 hours in Advance (6 - (AHI=1) = 5)\n",
    "    train_df = train_df, val_df = val_df, test_df = test_df # Create Tensorflow datasets for all 3 subsets\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use that larger dataset to do the modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1644/1644 [==============================] - 1s 681us/step - loss: 2.0492 - val_loss: 2.1636\n",
      "Epoch 2/20\n",
      "1644/1644 [==============================] - 1s 648us/step - loss: 2.0460 - val_loss: 2.1604\n",
      "Epoch 3/20\n",
      "1644/1644 [==============================] - 1s 654us/step - loss: 2.0459 - val_loss: 2.1488\n",
      "Epoch 4/20\n",
      "1644/1644 [==============================] - 1s 705us/step - loss: 2.0458 - val_loss: 2.1535\n",
      "Epoch 5/20\n",
      "1644/1644 [==============================] - 1s 736us/step - loss: 2.0458 - val_loss: 2.1525\n",
      "Epoch 6/20\n",
      "1644/1644 [==============================] - 1s 661us/step - loss: 2.0458 - val_loss: 2.1526\n",
      "Epoch 7/20\n",
      "1644/1644 [==============================] - 1s 667us/step - loss: 2.0458 - val_loss: 2.1529\n",
      "Epoch 8/20\n",
      "1644/1644 [==============================] - 1s 695us/step - loss: 2.0458 - val_loss: 2.1533\n",
      "Epoch 9/20\n",
      "1644/1644 [==============================] - 1s 674us/step - loss: 2.0458 - val_loss: 2.1530\n",
      "Epoch 10/20\n",
      "1644/1644 [==============================] - 1s 668us/step - loss: 2.0458 - val_loss: 2.1533\n",
      "Epoch 11/20\n",
      "1644/1644 [==============================] - 1s 679us/step - loss: 2.0458 - val_loss: 2.1534\n",
      "\n",
      "- Performance on *TRAINING* data:\n",
      "R2 = -7.45, RMSE = 1.71, MAE = 1.55, MAPE = 180.49%\n",
      "\n",
      "- Performance on *VALIDATION* data:\n",
      "R2 = -7.57, RMSE = 1.72, MAE = 1.56, MAPE = 183.37%\n",
      "\n",
      "- Performance on *TEST* data:\n",
      "R2 = -8.05, RMSE = 1.71, MAE = 1.55, MAPE = 143.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R2': -8.05, 'RMSE': 1.71, 'MAE': 1.55, 'MAPE': '143.0%'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the Model\n",
    "model = buildLinearModel(isBinary = False, label_width = 3)\n",
    "\n",
    "# Configure early stopping so we don't learn the training data too well at the expense of test/validation data (overfit)\n",
    "esCallback = tf.keras.callbacks.EarlyStopping(monitor='loss', mode=\"min\", patience=10, min_delta = 0.05)\n",
    "\n",
    "# NOTE: provide the Validation Dataset so that the Model does not check itself on Training Data\n",
    "model.fit(wg.train, validation_data = wg.val, callbacks = [esCallback], epochs = 20)\n",
    "\n",
    "print(\"\\r\\n- Performance on *TRAINING* data:\")\n",
    "evaluateRegressionModel(model, wg.train)\n",
    "print(\"\\r\\n- Performance on *VALIDATION* data:\")\n",
    "evaluateRegressionModel(model, wg.val)\n",
    "print(\"\\r\\n- Performance on *TEST* data:\")\n",
    "evaluateRegressionModel(model, wg.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
